name: ğŸ­ Streamlined Factory (Mass Data & Space Saver)

on:
  schedule:
    - cron: '0 */4 * * *' # 4ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (ë¹„ìš© ì ˆê°)
  push:
    branches: [ "main" ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  stream-production:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    # ----------------------------------------------------------------
    # [Step 1] ì´ˆê¸° ê³µê°„ í™•ë³´
    # ----------------------------------------------------------------
    - name: ğŸ§¹ Init Cleanup
      run: |
        echo "ğŸ§¹ Clearing initial space..."
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc
        docker system prune -af
        echo "âœ… Ready."

    # ----------------------------------------------------------------
    # [Step 2] í™˜ê²½ ì„¤ì •
    # ----------------------------------------------------------------
    - name: ğŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    # ----------------------------------------------------------------
    # [Step 3] ì§€ëŠ¥í˜• ìŠ¤íŠ¸ë¦¬ë° ìƒì‚° ì—”ì§„ (í•µì‹¬ ë¡œì§)
    # ----------------------------------------------------------------
    - name: ğŸ­ Run Stream Generator
      shell: bash
      run: |
        mkdir -p _artifacts_warehouse
        
        # íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±: ìƒì„± -> ì••ì¶• -> ì‚­ì œ ë¡œì§ í¬í•¨
        cat <<'EOF' > stream_generator.py
        import os
        import json
        import shutil
        import random
        import time
        from datetime import datetime

        # 1. ëŒ€ëŸ‰ ìƒì„±í•  ì„œë¹„ìŠ¤ ë° ì¹´í…Œê³ ë¦¬ ì •ì˜
        # ê° ì„œë¹„ìŠ¤ë³„ë¡œ ê³ ìœ í•œ ë°ì´í„° íŠ¹ì„±ì„ ì •ì˜í•©ë‹ˆë‹¤.
        CATEGORIES = {
            "Finance": ["Banking_Core", "Stock_Ledger", "Crypto_Vault", "Tax_Audit", "Insurance_Claim"],
            "Health": ["Bio_Genome", "Patient_Records", "Nano_Bot_Control", "Virus_Sim", "XRay_Archive"],
            "SmartCity": ["Traffic_Grid", "Power_Plant", "Water_Supply", "CCTV_Network", "Drone_Port"],
            "Space": ["Mars_Base", "Sat_Telemetry", "Rocket_Launch", "Deep_Space_Comms", "Alien_Signal"],
            "Military": ["Cyber_Defense", "Radar_Sys", "Missile_Guidance", "Troop_Log", "Sat_Spy"]
        }

        OUTPUT_DIR = "_artifacts_warehouse"

        def get_disk_free_mb():
            total, used, free = shutil.disk_usage("/")
            return free // (1024 * 1024)

        def generate_unique_data(service_name, category):
            """ê° ê²½ë¡œë³„ íŠ¹í™”ëœ ë°ì´í„° ìƒì„±"""
            timestamp = datetime.now().isoformat()
            
            # A. ì„¤ì • ë°ì´í„° (JSON)
            config_data = {
                "service": service_name,
                "category": category,
                "status": "ACTIVE",
                "shards": random.randint(1, 100),
                "created_at": timestamp,
                "security_level": "Level_5" if category == "Military" else "Level_3"
            }
            
            # B. ëŒ€ëŸ‰ ë”ë¯¸ ë°ì´í„° (CSV ì‹œë®¬ë ˆì´ì…˜) - ìš©ëŸ‰ ì œì–´
            # ì‹¤ì œë¡œëŠ” í¬ì§€ë§Œ ì••ì¶• íš¨ìœ¨ì´ ì¢‹ì€ ë°˜ë³µ í…ìŠ¤íŠ¸ ìƒì„±
            dummy_rows = []
            for i in range(500): # íŒŒì¼ ê°œìˆ˜/í¬ê¸° ì¡°ì ˆ
                dummy_rows.append(f"{i},{service_name}_hash_{random.getrandbits(32)},{category},active")
            
            return config_data, "\n".join(dummy_rows)

        def run_factory():
            total_services = sum(len(v) for v in CATEGORIES.values())
            print(f"ğŸš€ Starting Stream Production for {total_services} Services...")
            
            processed_count = 0
            
            for category, services in CATEGORIES.items():
                for service in services:
                    # [ì•ˆì „ì¥ì¹˜] ë””ìŠ¤í¬ ê³µê°„ ì²´í¬ (500MB ë¯¸ë§Œì´ë©´ ë¹„ìƒ ì •ì§€)
                    if get_disk_free_mb() < 500:
                        print("ğŸš¨ EMERGENCY: Low Disk Space! Stopping Generation.")
                        break

                    print(f"ğŸ› ï¸ [Processing] {category}/{service} ...")
                    
                    # 1. ì„ì‹œ ì‘ì—… ê²½ë¡œ ìƒì„± (src/Category/Service)
                    base_path = f"src/{category}/{service}"
                    os.makedirs(base_path, exist_ok=True)
                    
                    # 2. ë°ì´í„° ì ì¬ (Data Loading)
                    cfg, csv_data = generate_unique_data(service, category)
                    
                    # íŒŒì¼ ì“°ê¸°
                    with open(f"{base_path}/config.json", "w") as f:
                        json.dump(cfg, f, indent=4)
                    
                    with open(f"{base_path}/transaction_logs.csv", "w") as f:
                        f.write(csv_data)
                        
                    with open(f"{base_path}/README.md", "w") as f:
                        f.write(f"# {service}\n\nManaged by Stream Factory.\nCategory: {category}")
                    
                    # 3. [í•µì‹¬] ì••ì¶• ë° ê²©ë¦¬ (Compress to Artifact)
                    # src í´ë”ë¥¼ _artifacts_warehouse/Service.zip ìœ¼ë¡œ ì••ì¶•
                    shutil.make_archive(f"{OUTPUT_DIR}/{service}", 'zip', base_path)
                    
                    # 4. [í•µì‹¬] ì¦‰ì‹œ ì‚­ì œ (Flush) - ê³µê°„ í™•ë³´
                    shutil.rmtree(base_path)
                    
                    # ìƒìœ„ ì¹´í…Œê³ ë¦¬ í´ë”ë„ ë¹„ì—ˆìœ¼ë©´ ì‚­ì œ (ê¹”ë”í•˜ê²Œ ìœ ì§€)
                    try: os.rmdir(f"src/{category}")
                    except: pass
                    
                    processed_count += 1
                    
            print(f"âœ… Completed. {processed_count}/{total_services} services archived.")
            
            # ìµœì¢… ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
            with open(f"{OUTPUT_DIR}/_MANIFEST.txt", "w") as f:
                f.write(f"Total Processed: {processed_count}\nTimestamp: {datetime.now()}")

        if __name__ == "__main__":
            run_factory()
        EOF
        
        # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        python stream_generator.py

    # ----------------------------------------------------------------
    # [Step 4] ê²°ê³¼ë¬¼ ì—…ë¡œë“œ (ì••ì¶•ëœ íŒŒì¼ë“¤ë§Œ)
    # ----------------------------------------------------------------
    - name: ğŸ“¤ Upload Data Packages
      uses: actions/upload-artifact@v4
      with:
        name: mass-service-data
        path: _artifacts_warehouse/
        retention-days: 1 # ë³´ê´€ ë¹„ìš© ì ˆì•½
        if-no-files-found: warn

    # ----------------------------------------------------------------
    # [Step 5] ëŒ€ì‹œë³´ë“œ
    # ----------------------------------------------------------------
    - name: ğŸ“Š Summary Dashboard
      if: always()
      run: |
        echo "# ğŸ­ Stream Factory Report" >> $GITHUB_STEP_SUMMARY
        
        COUNT=$(ls _artifacts_warehouse/*.zip 2>/dev/null | wc -l)
        SIZE=$(du -sh _artifacts_warehouse | cut -f1)
        
        echo "### ğŸ“¦ Production Stats" >> $GITHUB_STEP_SUMMARY
        echo "- **Total Services Generated:** $COUNT" >> $GITHUB_STEP_SUMMARY
        echo "- **Total Artifact Size:** $SIZE" >> $GITHUB_STEP_SUMMARY
        echo "- **Method:** Generate -> Zip -> Flush (Zero Disk Waste)" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“‚ Generated Services (Sample)" >> $GITHUB_STEP_SUMMARY
        ls -1 _artifacts_warehouse/*.zip | head -n 10 | sed 's/_artifacts_warehouse\///g' | sed 's/^/- ğŸ“¦ /' >> $GITHUB_STEP_SUMMARY
