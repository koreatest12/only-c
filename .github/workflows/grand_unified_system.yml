name: ğŸ­ Massive Data Factory (Infinite Stream)

on:
  schedule:
    - cron: '30 */4 * * *' # 4ì‹œê°„ë§ˆë‹¤ ëŒ€ëŸ‰ ìƒì‚°
  push:
    branches: [ "main" ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  massive-production:
    runs-on: ubuntu-latest
    timeout-minutes: 45 # ëŒ€ëŸ‰ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‹œê°„ ì—°ì¥
    
    steps:
    # ----------------------------------------------------------------
    # [Step 1] í´ë¦° ë£¸ ì´ˆê¸°í™”
    # ----------------------------------------------------------------
    - name: ğŸ§¹ Deep Clean Init
      run: |
        echo "ğŸ§¹ Purging massive space..."
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /usr/share/swift
        docker system prune -af
        df -h
        echo "âœ… Space Secured."

    # ----------------------------------------------------------------
    # [Step 2] íŒŒì´ì¬ í™˜ê²½ (ë°ì´í„° ì—”ì§„)
    # ----------------------------------------------------------------
    - name: ğŸ Setup Python Engine
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    # ----------------------------------------------------------------
    # [Step 3] ì´ˆëŒ€í˜• ë°ì´í„° ìƒì„± ì—”ì§„ ê°€ë™ (í•µì‹¬)
    # ----------------------------------------------------------------
    - name: ğŸ­ Run Massive Data Engine
      shell: bash
      run: |
        mkdir -p _warehouse
        
        # íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸: ë³µì¡í•œ ë°ì´í„° êµ¬ì¡° ë° ëŒ€ìš©ëŸ‰ íŒŒì¼ ìƒì„± ë¡œì§
        cat <<'EOF' > massive_generator.py
        import os
        import json
        import shutil
        import random
        import time
        import uuid
        from datetime import datetime

        # 1. ì´ˆëŒ€í˜• ì¸í”„ë¼ ì •ì˜ (ì¹´í…Œê³ ë¦¬ > ë¦¬ì „ > ì„œë¹„ìŠ¤)
        INFRASTRUCTURE = {
            "Global_Finance": ["Payment_Gateway", "Ledger_Core", "Fraud_Detection", "HFT_Trading", "Settlement_Chain"],
            "Mega_Health": ["Genomic_Storage", "MRI_Archive", "Patient_DB", "Tele_Medicine", "Bio_Sensor_IoT"],
            "Hyper_Retail": ["Inventory_AI", "Logistics_Drone", "User_Behavior", "Global_Sales", "Recommendation_Engine"],
            "Smart_Defense": ["Sat_Recon", "Cyber_Warfare", "Drone_Swarm", "Encrypted_Comms", "Missile_Guidance"],
            "Future_Tech": ["Quantum_Sim", "Metaverse_Render", "NFT_Minting", "Fusion_Control", "Space_Telemetry"],
            "Legacy_Systems": ["Mainframe_COBOL", "Tape_Backup", "Old_Oracle", "Email_Archive", "Paper_Scan"]
        }
        
        REGIONS = ["US-East", "EU-Central", "Asia-NE", "SA-West"]
        OUTPUT_DIR = "_warehouse"

        def get_disk_free_mb():
            _, _, free = shutil.disk_usage("/")
            return free // (1024 * 1024)

        def create_heavy_files(path, service_name):
            """ë‹¤ì–‘í•œ í¬ë§·ì˜ ë¬´ê±°ìš´ ë°ì´í„° ìƒì„±"""
            
            # A. ëŒ€ìš©ëŸ‰ SQL ë¤í”„ (í…ìŠ¤íŠ¸ ë°˜ë³µ)
            with open(f"{path}/{service_name}_dump.sql", "w") as f:
                f.write(f"-- SQL DUMP FOR {service_name}\n")
                f.write("INSERT INTO logs (id, payload, ts) VALUES \n")
                # 10,000 ë¼ì¸ ìƒì„±
                chunk = f"('{uuid.uuid4()}', 'encrypted_payload_data_{random.getrandbits(64)}', '{datetime.now()}'),\n"
                f.write(chunk * 5000) 
            
            # B. ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (ë¹ ë¥¸ ìƒì„±)
            # 10MBì§œë¦¬ ë”ë¯¸ ë°”ì´ë„ˆë¦¬ íŒŒì¼ ìƒì„±
            with open(f"{path}/sensor_data.bin", "wb") as f:
                f.write(os.urandom(1024) * 1024) # 1MB (ì†ë„ ìœ„í•´ ì¡°ì ˆ)

            # C. JSON ì„¤ì • íŒŒì¼ (êµ¬ì¡°í™” ë°ì´í„°)
            config = {
                "service": service_name,
                "nodes": [f"node-{i}" for i in range(50)],
                "config_map": {f"key_{k}": random.getrandbits(32) for k in range(100)}
            }
            with open(f"{path}/config.json", "w") as f:
                json.dump(config, f, indent=2)

            # D. ì•¡ì„¸ìŠ¤ ë¡œê·¸ (ë¼ì¸ ë‹¨ìœ„ ë°ì´í„°)
            with open(f"{path}/access.log", "w") as f:
                log_line = '192.168.1.{} - - [{}] "GET /api/v1/{} HTTP/1.1" 200 4096\n'
                for _ in range(2000):
                    f.write(log_line.format(random.randint(1,255), datetime.now(), service_name))

        def run_massive_factory():
            total_tasks = sum(len(v) for v in INFRASTRUCTURE.values()) * len(REGIONS)
            print(f"ğŸš€ Initializing Massive Factory: {total_tasks} Workloads Expected...")
            
            processed = 0
            
            for category, services in INFRASTRUCTURE.items():
                for region in REGIONS:
                    # ì¹´í…Œê³ ë¦¬/ë¦¬ì „ í´ë” ìƒì„±
                    region_base = f"data_lake/{category}/{region}"
                    
                    for service in services:
                        # [ì•ˆì „ì¥ì¹˜] ë””ìŠ¤í¬ ë¶€ì¡± ì‹œ ë¹„ìƒ ì •ì§€
                        if get_disk_free_mb() < 800:
                            print("ğŸš¨ CRITICAL: Low Disk Space (<800MB). Halting Production.")
                            return

                        print(f"âš¡ [Gen] {category} > {region} > {service}")
                        
                        service_path = f"{region_base}/{service}"
                        os.makedirs(service_path, exist_ok=True)
                        
                        # 1. ëŒ€ëŸ‰ ë°ì´í„° íŒŒì¼ ìƒì„±
                        create_heavy_files(service_path, service)
                        
                        # 2. ë©”íƒ€ë°ì´í„° ìƒì„±
                        with open(f"{service_path}/_META.txt", "w") as f:
                            f.write(f"Created: {datetime.now()}\nType: Massive_Batch\nPath: {service_path}")

                        # 3. [í•µì‹¬] ì••ì¶• (Path-Specific Archiving)
                        # data_lake/Finance/US-East/Service -> _warehouse/Finance_US-East_Service.zip
                        zip_name = f"{category}_{region}_{service}"
                        shutil.make_archive(f"{OUTPUT_DIR}/{zip_name}", 'zip', service_path)
                        
                        # 4. [í•µì‹¬] ì¦‰ì‹œ ì‚­ì œ (Flush) - ê³µê°„ ë°˜í™˜
                        shutil.rmtree(service_path)
                        processed += 1
                        
                    # ë¦¬ì „ ì²˜ë¦¬ê°€ ëë‚˜ë©´ ìƒìœ„ í´ë”ë„ ì •ë¦¬ (ë¹ˆ í´ë” ì‚­ì œ)
                    try: shutil.rmtree(region_base) 
                    except: pass
            
            print(f"âœ… MISSION COMPLETE. {processed} Massive Packages Created.")
            
            # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
            with open(f"{OUTPUT_DIR}/_MANIFEST.json", "w") as f:
                json.dump({"total_files": processed, "timestamp": str(datetime.now())}, f)

        if __name__ == "__main__":
            run_massive_factory()
        EOF
        
        # ì—”ì§„ ì‹¤í–‰
        python massive_generator.py

    # ----------------------------------------------------------------
    # [Step 4] ê²°ê³¼ë¬¼ ì—…ë¡œë“œ (ê¸°ê°€ë°”ì´íŠ¸ê¸‰ ë°ì´í„°ë„ ì•ˆì „í•˜ê²Œ ë¶„í•  ì—…ë¡œë“œ)
    # ----------------------------------------------------------------
    - name: ğŸ“¤ Upload Massive Artifacts
      uses: actions/upload-artifact@v4
      with:
        name: massive-data-stream
        path: _warehouse/
        retention-days: 1
        compression-level: 0 # ì´ë¯¸ ì••ì¶•í–ˆìœ¼ë¯€ë¡œ ì†ë„ë¥¼ ìœ„í•´ ì••ì¶• ì•ˆ í•¨

    # ----------------------------------------------------------------
    # [Step 5] ëŒ€ì‹œë³´ë“œ ë¦¬í¬íŠ¸
    # ----------------------------------------------------------------
    - name: ğŸ“Š Factory Report
      if: always()
      run: |
        echo "# ğŸ­ Massive Data Factory Report" >> $GITHUB_STEP_SUMMARY
        
        COUNT=$(ls _warehouse/*.zip 2>/dev/null | wc -l)
        SIZE=$(du -sh _warehouse | cut -f1)
        DISK_END=$(df -h / | awk 'NR==2 {print $4}')
        
        echo "### ğŸ“ˆ Production Metrics" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
        echo "| :--- | :--- |" >> $GITHUB_STEP_SUMMARY
        echo "| **Total Packages** | **$COUNT** (Services * Regions) |" >> $GITHUB_STEP_SUMMARY
        echo "| **Total Size** | **$SIZE** |" >> $GITHUB_STEP_SUMMARY
        echo "| **Final Disk Free** | **$DISK_END** (Safe) |" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ğŸ“‚ Generated Data Categories" >> $GITHUB_STEP_SUMMARY
        echo "- **Global Finance** (SQL, Logs)" >> $GITHUB_STEP_SUMMARY
        echo "- **Mega Health** (Bio-sensor Data)" >> $GITHUB_STEP_SUMMARY
        echo "- **Hyper Retail** (Transaction CSV)" >> $GITHUB_STEP_SUMMARY
        echo "- **Smart Defense** (Binary Streams)" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "> All data generated, archived, and flushed in real-time." >> $GITHUB_STEP_SUMMARY
